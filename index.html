<html>
	<head>
		<title>Practice</title>
	</head>
	<body>
		<li>Instructions:</li>
		<ol><ul>The OCP cluster API endpoints is https://api.ocp4.example.com:6443 and the web console is available at 
					https://console-openshift-console.apps.ocp4.example.com. </ul><ul>You can
					access the cluster using the account admin with redhatocp as the password. </ul><ul>
			There is a developer account with developer as the password. </ul><ul>
			The registry server is available from registry.lab.example.com for container images.You can access the server using developer account with the password of developer.</ul><ul>
			The oc command is available on the workstation machine.</ul></ol><br>
<li> 
	1 - Deploy OpenShift Virtualization<ol>
	Install OpenShift Virtualization in openshift-cnv project. Install and configure kubevirt-converged in the same project.
	</ol></li><br>
<li>	
	2 - Resume a node from maintenance mode<ol>
		The node must be able to schedule pods and should be in a ready state.</ol>
</li>
		<li>
			3 - Deploy a VM and configure access to it<ol><ul>
					* Deploy RHEL9.2 in a project ramtops.</ul><ul>
					* The user verence-ii should be able to create and update the virtual machine in ramtops project.</ul><ul>
					* The image is http://utility.lab.example.com:8080/openshift4/images/rhel-9.2-x86_64-vm.qcow2.</ul><ul>
					* The persistent storage is 30Gi the name is lancre.</ul><ul>
					* The storage class to be used ocs-external-storagecluster-ceph-rbd.</ul><ul>
					* The flavour is small, type server. </ul><ul>
					* The virtual machine should have a user verence-ii with the password as redhatocp. This has to be done using cloud-init.</ul><ul>
					* SSH must be used from the /home/opsadm/.ssh/id_rsa_ex316.pub.</ul><ul>
					* A user moist can view the metrics of the virtual machine.</ul><ul>
					* The user moist should also be able to start,stop,pause and restart the virtual machine.</ul></ol>
		</li>
		<li>
			4 - Configure a web server in a VM<ol><ul>
					* The lancre virtual machine should have httpd package installed.</ul><ul>
	* The service of httpd should be started and enabled.</ul><ul>
	* The web server should serve a file service.html from registry.lab.example.com/service.html.</ul><ul>
	* The service.html must exist in /var/www/html directory and should be served to the pods.</ul><ul>
	* Th ClusterIP service should be configured such that the endpoints is able to serve.</ul><ul>
	* create a Network policy allow-web-access.</ul><ul>
	* The Network Policy should restrict the access to the ramtops project.</ul><ul>
	* Only access to port 80.</ul><ul>
        * No other projects should be allowed to access the lancre virtual machine as per the network policy.</ul><ul>
	* The installation of the required packages can be done using the repository as defined in http://registry.lab.example.com/myrepo/rhel.repo.<</ul></ol>
</li>
		<li>
			5 - Deploy a multihomed VM<ol><ul>
        * The user moist should be able to create and update the virtual machine in uberwald project.</ul><ul>
	* the VM ligpiw runs in a uberwald.</ul><ul>
	* The persistent storage is 30Gi the name is lancre.</ul><ul>
        * The storage class to be used ocs-external-storagecluster-ceph-rbd.</ul><ul>
	* The flavour is small, type server. </ul><ul>
	* The virtual machine should have a user verence-ii with the password as redhatocp. This has to be done using cloud-init. </ul><ul>
	* SSH must be used from the /home/opsadm/.ssh/id_rsa_ex316.pub.</ul><ul>
        * the virtual machine should have two network interfaces. The first network interface is the default using the pod network as masquerade.</ul><ul>
	* the second interface should use the named nic-0 which is connected to the bridge which is using the uberwald/ankhold network attachment definition resource.</ul></ol>
</li>
		<li>
			6 - Manage storage for VM<ol><ul>
	* Create a VM dalwin using the dalwin templates.</ul><ul>
	* Create a VM fili using the fili templates.</ul><ul>
	* The VMs should be in the project vm-import.</ul><ul>
	* Move the webdata disk from dalwin to fili without loosing the data and ensuring that the access mode, volume mode and storage class remains the same.</ul><ul>
        * On the fili VM, the device should be shown as /dev/vdc and mount it permanently to /var/www/html.</ul><ul>
	* On the dalwin VM, add additional disk using the 1GiB size from ocs-external-storagecluster-ceph-rbd with shared access. </ul><ul>
	* The new disk on dalwin VM should be identified as /dev/vdc and should be mounted persistently under /var/www/html. </ul></ol>
	</li>
		<li>
			7 - Create a VM Template<ol><ul>
	* Create a project vt100cinema.</ul><ul>
	* Create a template named docusri in the vt100cinema project using the rhel9-server-small image.</ul><ul>
	* Configure the scheduling to support Live Migration.</ul><ul>
	* Ensure that the VM uses the default interface from the pod networking and is set to Masquerade.</ul><ul>
	* The system should have a cloudinitdisk and a rootdisk using the http://utility.lab.example.com:8080/openshift4/images/rhel-9.2-x86_64-vm.qcow2 image. </ul><ul>
	* The rootdisk should be of 30 GiB based on virtio model and should use ocs-external-storagecluster-ceph-rbd storageclass.</ul><ul>
	* It should have shared access and volume mode should be block.</ul><ul>
	* The template should incude a SSH public key from /home/opsadm/.ssh/id_rsa_ex316.pub.</ul><ul>
	* The VM initialization should have the installation of vt100cinema package using the repository as defined in http://registry.lab.example.com/myrepo/rhel.repo.</ul><ul>
	* The template should include a cloudinit user named gandalf with the password of redhatocp.</ul></ol>
	</li>
		<li>
			8 - Create a service using a VM Load Balancing<ol><ul>
	* create 2 VMs using a docusri template named as castor and pollux in the vt100cinema project.</ul><ul>
	* Create the service webservice of the type NodePort with the pod Selector as app:flix.</ul><ul>
	* The webservice should be listening on port 23 and NodePort should be 30023. The sample template file is provided.</ul><ul>
        * Create a route called webroute so that the webservice is accessible using the link flix-webservice-vt100cinema.apps.ocp4.example.com.</ul><ul>
	* You can use telnet flix-webservice-vt100cinema.apps.ocp4.example.com 30023 to test.</ul></ol>
	</li>
		<li>
			9 - Create a VM snapshot<ol><ul>
	* Create a volume snapshot of the PVC associated with the pollux VM from the project vt100cinema with the name as pollux-freeze.</ul><ul>
	* Ensure that the VM was running successfully and the disk was accessible before taking the snapshot.</ul></ol>
		<li>
			10 - Configure VM migration<ol><ul>
        * The VM named ligpiw from uberwald project is schedule to use the nodes which matches the label uempires=twobats.</ul><ul>
        * Ensure that the VM will migrate between those nodes and not in any other nodes.</ul></ol>
</li>
		<li>
			11 - VM Cloning<ol><ul>
	* Prepare the castor VM for cloning such that it uses the name castor-copy.</ul><ul>
	* The PVC castor cloned using the datavolumes. </ul></ol>
</li>
		<li>
			12 - Configure Liveness probe<ol><ul>
	* The ligpiw VM should have mariadb-server package installed.</ul><ul>
	* The mariadb service should be enabled and started.</ul><ul>
	* The mariadb service should listen on tcp port 3306.</ul><ul>
	* Configure livenessprobe for the VM such that it send requests to the TCP socket on port 3306.</ul><ul>
	* The probing of the VM should start after 120 seconds.</ul><ul>
	* The probing should wait for 5 seconds to finish.</ul><ul>
	* The installation of the required packages can be done using the repository as defined in http://registry.lab.example.com/myrepo/rhel.repo.</ul></ol>
	</li>
		<li>
			13 - Prepare a VM from a node failure<ol><ul>
	 * Create a VM named suviva in the project suviva using the template suviva.</ul><ul>
	 * The suviva VM should be scheduled to only run on master1 and master2.</ul><ul>
	 * The suviva VM must migrate automatically to another node in case of failure.</ul></ol>
		</li><li>
	14 - Migrate Virtual Machines from Compatible Hypervisors<ol><ul>
	* The rhel9-web VM image is available on the utility.lab.example.com:/exports-ocp4/ova.</ul><ul>
	* The VM has access to one network which should be associated with the default pod network.</ul><ul>
	* The VM should use the ocs-external-storagecluster-ceph-rbd as the target storageclass.</ul><ul>
	* The rhel9-web VM should be migrated to the web-import project.</ul><ul>
	* The rhel9-web VM must be running in the web-import project.</ul></ol></li>
	</body>
</html>
