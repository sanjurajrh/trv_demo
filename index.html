<html>
	<head>
		<title>Practice</title>
	</head>
	<body>
		<li> Instructions:</li>
		<li> The OCP cluster API endpoints is https://api.ocp4.example.com:6443 and the web console is available at 
			https://console-openshift-console.apps.ocp4.example.com. You can
			access the cluster using the account admin with redhatocp as the password. 
			There is a developer account with developer as the password. 
			The oc command is available on the workstation machine. The machines worker01 and work02 are assigned with a label orgnet=true</li>
<li> 
1 - Deploy OpenShift Virtualization
	Install OpenShift Virtualization in openshift-cnv project. Install and configure kubevirt-converged in the same project.
			<br></li>
<li>	
2 - Resume a node from maintenance mode
	The node must be able to schedule pods and should be in a ready state.
</li>
		<li>
3 - Deploy a VM and configure access to it
	* Deploy RHEL9.2 in a project ramtops.
        * The user verence-ii should be able to create and update the virtual machine in ramtops project.
        * The image is http://utility.lab.example.com:8080/openshift4/images/rhel-9.2-x86_64-vm.qcow2.
	* The persistent storage is 30Gi the name is lancre.
        * The storage class to be used ocs-external-storagecluster-ceph-rbd.
	* The flavour is small, type server. 
	* The virtual machine should have a user verence-ii with the password as redhatocp. This has to be done using cloud-init. 
	* SSH must be used from the /home/opsadm/.ssh/id_rsa_ex316.pub.
	* A user moist can view the metrics of the virtual machine. 
	* The user moist should also be able to start,stop,pause and restart the virtual machine.
		</li>
		<li>
4 - Configure a web server in a VM
        * The lancre virtual machine should have httpd package installed. 
	* The service of httpd should be started and enabled.
	* The web server should serve a file service.html from registry.lab.example.com/service.html.
	* The service.html must exist in /var/www/html directory and should be served to the pods.
	* Th ClusterIP service should be configured such that the endpoints is able to serve.
	* create a Network policy allow-web-access.
	* The Network Policy should restrict the access to the ramtops project.
	* Only access to port 80.
        * No other projects should be allowed to access the lancre virtual machine as per the network policy.
	* The installation of the required packages can be done using the repository as defined in http://registry.lab.example.com/myrepo/rhel.repo.
</li>
		<li>
5 - Deploy a multihomed VM
        * The user moist should be able to create and update the virtual machine in uberwald project.
	* the VM ligpiw runs in a uberwald.
	* The persistent storage is 30Gi the name is lancre.
        * The storage class to be used ocs-external-storagecluster-ceph-rbd.
	* The flavour is small, type server. 
	* The virtual machine should have a user verence-ii with the password as redhatocp. This has to be done using cloud-init. 
	* SSH must be used from the /home/opsadm/.ssh/id_rsa_ex316.pub.
        * the virtual machine should have two network interfaces. The first network interface is the default using the pod network as masquerade.
        * the second interface should use the named nic-0 which is connected to the bridge which is using the uberwald/ankhold network attachment definition resource.
</li>
		<li>
6 - Manage storage for VM
	* Create a VM dalwin using the dalwin templates.
	* Create a VM fili using the fili templates.
	* The VMs should be in the project vm-import.
	* Move the webdata disk from dalwin to fili without loosing the data and ensuring that the access mode, volume mode and storage class remains the same.
        * On the fili VM, the device should be shown as /dev/vdc and mount it permanently to /var/www/html.
	* On the dalwin VM, add additional disk using the 1GiB size from ocs-external-storagecluster-ceph-rbd with shared access. 
	* The new disk on dalwin VM should be identified as /dev/vdc and should be mounted persistently under /var/www/html. 
	</li>
		<li>
7 - Create a VM Template
	* Create a project vt100cinema.
	* Create a template named docusri in the vt100cinema project using the rhel9-server-small image.
	* Configure the scheduling to support Live Migration.
	* Ensure that the VM uses the default interface from the pod networking and is set to Masquerade.
	* The system should have a cloudinitdisk and a rootdisk using the http://utility.lab.example.com:8080/openshift4/images/rhel-9.2-x86_64-vm.qcow2 image. 
	* The rootdisk should be of 30 GiB based on virtio model and should use ocs-external-storagecluster-ceph-rbd storageclass.
	* It should have shared access and volume mode should be block.
	* The template should incude a SSH public key from /home/opsadm/.ssh/id_rsa_ex316.pub.
	* The VM initialization should have the installation of vt100cinema package using the repository as defined in http://registry.lab.example.com/myrepo/rhel.repo.
	* The template should include a cloudinit user named gandalf with the password of redhatocp.
	</li>
		<li>
8 - Create a service using a VM Load Balancing
	* create 2 VMs using a docusri template named as castor and pollux in the vt100cinema project.
	* Create the service webservice of the type NodePort with the pod Selector as app:flix.
	* The webservice should be listening on port 23 and NodePort should be 30023. The sample template file is provided.
        * Create a route called webroute so that the webservice is accessible using the link flix-webservice-vt100cinema.apps.ocp4.example.com.
	* You can use telnet flix-webservice-vt100cinema.apps.ocp4.example.com 30023 to test.
	</li>
		<li>
9 - Create a VM snapshot
	* Create a volume snapshot of the PVC associated with the pollux VM from the project vt100cinema with the name as pollux-freeze.
	* Ensure that the VM was running successfully and the disk was accessible before taking the snapshot.
		<li>
10 - Configure VM migration
        * The VM named ligpiw from uberwald project is schedule to use the nodes which matches the label uempires=twobats.
        * Ensure that the VM will migrate between those nodes and not in any other nodes.
</li>
		<li>
11 - VM Cloning
	* Prepare the castor VM for cloning such that it uses the name castor-copy.
	* The PVC castor cloned using the datavolumes. 
</li>
		<li>
12 - Configure Liveness probe
	* The ligpiw VM should have mariadb-server package installed.
	* The mariadb service should be enabled and started.
	* The mariadb service should listen on tcp port 3306.
	* Configure livenessprobe for the VM such that it send requests to the TCP socket on port 3306.
	* The probing of the VM should start after 120 seconds.
	* The probing should wait for 5 seconds to finish.
	* The installation of the required packages can be done using the repository as defined in http://registry.lab.example.com/myrepo/rhel.repo.
	</li>
		<li>
13 - Prepare a VM from a node failure
	 * Create a VM named suviva in the project suviva using the template suviva.
	 * The suviva VM should be scheduled to only run on master1 and master2.
	 * The suviva VM must migrate automatically to another node in case of failure.
	</li>
14 - Migrate Virtual Machines from Compatible Hypervisors
	* The rhel9-web VM image is available on the utility.lab.example.com:/exports-ocp4/ova.
	* The VM has access to one network which should be associated with the default pod network .
	* The VM should use the ocs-external-storagecluster-ceph-rbd as the target storageclass.
	* The rhel9-web VM should be migrated to the web-import project.
	* The rhel9-web VM must be running in the web-import project.
	</body>
</html>
